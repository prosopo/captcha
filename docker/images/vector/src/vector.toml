# Vector Configuration for Prosopo Captcha Provider Logging
# 
# This configuration file sets up Vector (a log aggregation tool) to collect and forward
# logs from various Docker containers to OpenObserve (oo) for centralized logging and monitoring.
# 
# The pipeline consists of:
# 1. Sources: Collect logs from Docker containers and events
# 2. Transforms: Process and format the collected data
# 3. Sinks: Forward processed data to OpenObserve instances
#
# Environment Variables Required:
# - OO_HOST: Host identifier for the machine
# - NODE_ENV: Environment (staging, production, etc.)
# - OO_USERNAME: OpenObserve authentication username
# - OO_PASSWORD: OpenObserve authentication password
# - OO2_USERNAME: Secondary OpenObserve authentication username
# - OO2_PASSWORD: Secondary OpenObserve authentication password
# - MONGO_INITDB_ROOT_USERNAME: MongoDB root username
# - MONGO_INITDB_ROOT_PASSWORD: MongoDB root password

# =============================================================================
# CONFIGURATION NOTES
# =============================================================================
#
# Rate Limiting: Set to 10 requests per second to avoid overwhelming the API
# Batch Timeout: 15 seconds to balance between latency and throughput
# Buffer Size: ~256MB disk buffer to handle temporary network issues
# Compression: Gzip enabled to reduce bandwidth usage
# Health Checks: Enabled to monitor sink connectivity
#
# To enable console output for debugging, uncomment the console sink sections
# and comment out the corresponding OpenObserve sinks.

# =============================================================================
# API CONFIGURATION
# =============================================================================

# Enable Vector's API for monitoring and management
[api]
enabled = true
# Restrict API access to localhost only for security
address = "127.0.0.1:8686"

# =============================================================================
# PROVIDER LOGS PIPELINE
# =============================================================================

# Source: Collect logs from Docker containers labeled with vector.provider=true
# This captures application logs from the captcha provider services
[sources.provider]
type = "docker_logs"
include_labels = ["vector.provider=true"]

# Transform: Format provider logs for consistent structure
# - Parses JSON messages if available, otherwise treats as plain text
# - Adds metadata like host, container name, image, and stream type
[transforms.provider_format]
type = "remap"
inputs = ["provider"]
source = '''
output={}
parsed=parse_json!(string!(.message) ?? "") ?? null
if parsed != null {
    output = parsed
} else {
    output.message = .message
}
output.host=get_env_var!("OO_HOST")
output.container_name=.container_name
output.image=.image
output.stream=.stream
output.source_type=.source_type
.=output
'''

# Sink: Forward provider logs to primary OpenObserve instance (oo.prosopo.io)
# Configuration includes authentication, batching, rate limiting, and buffering
[sinks.oo_provider]
type = "http"
inputs = ["provider_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_node/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488
compression = "gzip"

# Sink: Forward provider logs to secondary OpenObserve instance (oo2.prosopo.io)
# Backup/fallback logging destination for redundancy
[sinks.oo2_provider]
type = "http"
inputs = ["provider_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_node/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488
compression = "gzip"

# =============================================================================
# MONGO LOGS PIPELINE
# =============================================================================

# Source: Collect logs from Docker containers labeled with vector.mongo=true
[sources.mongo]
type = "docker_logs"
include_labels = ["vector.mongo=true"]

# Transform: Format mongo logs for consistent structure
# - Parses JSON messages if available, otherwise treats as plain text
# - Adds metadata like host, container name, image, and stream type
[transforms.mongo_format]
type = "remap"
inputs = ["mongo"]
source = '''
output={}
parsed=parse_json!(string!(.message) ?? "") ?? null
if parsed != null {
    output = parsed
} else {
    output.message = .message
}
output.host=get_env_var!("OO_HOST")
output.container_name=.container_name
output.image=.image
output.stream=.stream
output.source_type=.source_type
.=output
'''

# Sink: Forward mongo logs to primary OpenObserve instance (oo.prosopo.io)
# Configuration includes authentication, batching, rate limiting, and buffering
[sinks.oo_mongo]
type = "http"
inputs = ["mongo_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_mongo/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488
compression = "gzip"

# Sink: Forward mongo logs to secondary OpenObserve instance (oo2.prosopo.io)
# Backup/fallback logging destination for redundancy
[sinks.oo2_mongo]
type = "http"
inputs = ["mongo_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_mongo/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488
compression = "gzip"

# =============================================================================
# DOCKER EVENTS PIPELINE
# =============================================================================

# Source: Monitor Docker container lifecycle events
# Executes docker events command to capture container start/stop/restart events
# Only monitors containers labeled with vector.docker=true
[sources.docker]
type = "exec"
# Docker events documentation: https://docs.docker.com/reference/cli/docker/system/events/#containers
# Event types being monitored:
# - start: Container has been started
# - stop: Container has been stopped
# - restart: Container has been restarted
# - die: Container has stopped running (crash or intentional termination)
# - pause: Container is paused (not actively running but not stopped)
# - unpause: Container has resumed running after being paused
# - kill: Container has been forcefully terminated
# - oom: Container was terminated due to running out of memory
command = [ "docker", "events", "--format", "json", "--filter", "label=vector.docker=true", "--filter", "event=start", "--filter", "event=stop", "--filter", "event=restart", "--filter", "event=die", "--filter", "event=pause", "--filter", "event=unpause", "--filter", "event=kill", "--filter", "event=oom" ]
mode = "streaming"
decoding.codec = "json"

# Transform: Format Docker events for consistent structure
# - Parses JSON messages and adds host identification
# - Extracts container metadata for better observability
[transforms.docker_format]
type = "remap"
inputs = ["docker"]
source = '''
output={}
parsed=parse_json!(string!(.message) ?? "") ?? null
if parsed != null {
    output = parsed
} else {
    output.message = .message
}
output.host=get_env_var!("OO_HOST")
output.container_name=.container_name
output.image=.image
output.stream=.stream
output.source_type=.source_type
output.name=.Actor.Attributes.name
.=output
'''

# Sink: Forward Docker events to primary OpenObserve instance
[sinks.oo_docker]
type = "http"
inputs = ["docker_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_docker/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# Sink: Forward Docker events to secondary OpenObserve instance
[sinks.oo2_docker]
type = "http"
inputs = ["docker_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_docker/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# =============================================================================
# CADDY WEB SERVER LOGS PIPELINE
# =============================================================================

# Source: Collect logs from Caddy web server containers
# Caddy is used as a reverse proxy and load balancer
[sources.caddy]
type = "docker_logs"
include_labels = ["vector.caddy=true"]

# Transform: Format Caddy logs for consistent structure
# - Parses JSON messages and adds host identification
# - Extracts request ID from response headers for request tracing
[transforms.caddy_format]
type = "remap"
inputs = ["caddy"]
source = '''
output={}
parsed=parse_json!(string!(.message) ?? "") ?? null
if parsed != null {
    output = parsed
} else {
    output.message = .message
}
output.host=get_env_var!("OO_HOST")
output.container_name=.container_name
output.image=.image
output.stream=.stream
output.source_type=.source_type
output.request_id = get(.resp_headers, ["X-Request-Id"])[0] ?? null
.=output
'''

# Sink: Forward Caddy logs to primary OpenObserve instance
[sinks.oo_caddy]
type = "http"
inputs = ["caddy_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_caddy/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# Sink: Forward Caddy logs to secondary OpenObserve instance
[sinks.oo2_caddy]
type = "http"
inputs = ["caddy_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_caddy/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# =============================================================================
# VECTOR INTERNAL LOGS PIPELINE
# =============================================================================

# Source: Collect Vector's own internal logs
# This helps monitor Vector's health and troubleshoot Vector-related issues
[sources.vector]
type = "internal_logs"

# Transform: Add host identification to Vector logs
[transforms.vector_format]
type = "remap"
inputs = ["vector"]
source = '''
.host=get_env_var!("OO_HOST")
'''

# Sink: Forward Vector logs to primary OpenObserve instance
[sinks.oo_vector]
type = "http"
inputs = ["vector_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_vector/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# Sink: Forward Vector logs to secondary OpenObserve instance
[sinks.oo2_vector]
type = "http"
inputs = ["vector_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_vector/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# =============================================================================
# REDIS LOGS PIPELINE
# =============================================================================

# Source: Collect Redis logs from Docker containers
# Note: This source is incorrectly configured as "internal_logs" - should be "docker_logs"
# with appropriate labels for Redis containers
[sources.redis]
type = "docker_logs"
include_labels = ["vector.redis=true"]

# Transform: Parse and format Redis logs
# Redis logs have a specific format that needs parsing for better observability
[transforms.redis_format]
type = "remap"
inputs = ["redis"]
source = '''
# Example Redis line:
# 12345:M 29 Aug 2023 12:34:56.789 * Ready to accept connections

# Start with a clean object
. = {}

# Keep the raw line just in case
.raw = string!(.message)

# Regex parse pieces from the raw line
match = parse_regex!(.raw, r'^(?P<pid>\d+):(?P<role>[A-Z])\s+(?P<day>\d{1,2})\s+(?P<mon>[A-Za-z]{3})\s+(?P<year>\d{4})\s+(?P<time>\d{2}:\d{2}:\d{2}\.\d{3})\s+(?P<level_symbol>.)\s+(?P<msg>.*)$')

.pid          = to_int!(match.pid)
.role         = match.role            # e.g. M, C, S, R (master/server, child, slave/replica, sentinel)
.level_symbol = match.level_symbol    # e.g. "*", "#", "X", "-"
.message      = match.msg

# Optional: map role letters to friendly names
.role_name = if .role == "M" { "server" } else if .role == "C" { "child" } else if .role == "S" { "replica" } else if .role == "R" { "sentinel" } else { "unknown" }

# Optional: map symbols to levels (adjust to your policy)
# Many folks treat:
# "*" as info/notice, "#" as warning, "-" as notice, and "X" as crit.
.level = if .level_symbol == "*" { "info" } else if .level_symbol == "#" { "warn" } else if .level_symbol == "X" { "error" } else { "notice" }

# Build timestamp
ts_str = string(match.day) + " " + string(match.mon) + " " + string(match.year) + " " + string(match.time)
.timestamp = parse_timestamp!(ts_str, "%d %b %Y %T%.3f")

.host=get_env_var!("OO_HOST")
'''

# Sink: Forward Redis logs to primary OpenObserve instance
[sinks.oo_redis]
type = "http"
inputs = ["redis_format"]
uri = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/${NODE_ENV:?err}_provider_redis/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# Sink: Forward Redis logs to secondary OpenObserve instance
[sinks.oo2_redis]
type = "http"
inputs = ["redis_format"]
uri = "https://oo2.prosopo.io/api/default/${NODE_ENV:?err}_provider_redis/_json"
method = "post"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
compression = "gzip"
encoding.codec = "json"
encoding.timestamp_format = "rfc3339"
healthcheck.enabled = true
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# =============================================================================
# METRICS PIPELINE
# =============================================================================
#
# This section collects various system and application metrics for monitoring
# and alerting purposes. Metrics are collected from multiple sources and
# forwarded to OpenObserve using Prometheus Remote Write protocol.

# =============================================================================
# HOST METRICS
# =============================================================================

# Source: Collect system-level metrics from the host machine
# Includes CPU, memory, disk, network, and other hardware metrics
[sources.host_metrics]
type = "host_metrics"
scrape_interval_secs = 15

# Transform: Add host and environment tags to host metrics
# Enables filtering and grouping metrics by host and environment
[transforms.host_metrics_format]
type = "remap"
inputs = ["host_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# VECTOR INTERNAL METRICS
# =============================================================================

# Source: Collect Vector's own internal metrics
# Includes performance metrics, error rates, and pipeline statistics
[sources.vector_metrics]
type = "internal_metrics"
scrape_interval_secs = 15

# Transform: Add host and environment tags to Vector metrics
# Helps monitor Vector's health and performance across different environments
[transforms.vector_metrics_format]
type = "remap"
inputs = ["vector_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# CADVISOR CONTAINER METRICS
# =============================================================================

# Source: Collect container metrics from cAdvisor
# cAdvisor provides detailed container resource usage metrics
# Endpoint: cAdvisor exposes metrics on configurable port
[sources.cadvisor_metrics]
type = "prometheus_scrape"
endpoints = [ "http://cadvisor:8080/metrics" ]
scrape_interval_secs = 15

# Transform: Add host and environment tags to cAdvisor metrics
# Enables container-level monitoring and resource tracking
[transforms.cadvisor_metrics_format]
type = "remap"
inputs = ["cadvisor_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# CADDY METRICS
# =============================================================================

# Source: Collect metrics from caddy instance
# Caddy exposes metrics in Prometheus format on a configurable port
[sources.caddy_metrics]
type = "prometheus_scrape"
endpoints = ["http://caddy:9090/metrics"]
scrape_interval_secs = 15

# Transform: Add host and environment tags to Caddy metrics
[transforms.caddy_metrics_format]
type = "remap"
inputs = ["caddy_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# REDIS METRICS
# =============================================================================

# Source: Collect metrics from Redis instance
# Monitors Redis performance, memory usage, and connection statistics
# Uses redis-exporter to expose Redis metrics in Prometheus format
[sources.redis_metrics]
type = "prometheus_scrape"
endpoints = [ "http://redis-exporter:9121/metrics" ]
scrape_interval_secs = 15

# Transform: Add host and environment tags to Redis metrics
[transforms.redis_metrics_format]
type = "remap"
inputs = ["redis_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# MONGODB METRICS
# =============================================================================

# Source: Collect metrics from MongoDB instance
# This source uses the mongodb_metrics Vector source to scrape metrics from the MongoDB database.
[sources.mongodb_metrics]
type = "prometheus_scrape"
endpoints = [ "http://database:9216/metrics" ]
scrape_interval_secs = 15

# Transform: Add host and environment tags to MongoDB metrics
[transforms.mongodb_metrics_format]
type = "remap"
inputs = ["mongodb_metrics"]
source = '''
.tags.host=get_env_var!("OO_HOST")
.tags.env=get_env_var!("NODE_ENV")
'''

# =============================================================================
# METRICS SINKS
# =============================================================================

# Sink: Forward all metrics to primary OpenObserve instance
# Uses Prometheus Remote Write protocol for efficient metric transmission
[sinks.oo_metrics]
type = "prometheus_remote_write"
inputs = ["host_metrics_format", "vector_metrics_format", "cadvisor_metrics_format", "mongodb_metrics_format", "redis_metrics_format", "caddy_metrics_format"]
endpoint = "https://oo.prosopo.io/api/dev_organization_29569_u24VnjrjN7XrP35/prometheus/api/v1/write"
auth.strategy = "basic"
auth.user = "${OO_USERNAME:?err}"
auth.password = "${OO_PASSWORD:?err}"
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488

# Sink: Forward all metrics to secondary OpenObserve instance
# Backup destination for metrics redundancy and disaster recovery
[sinks.oo2_metrics]
type = "prometheus_remote_write"
inputs = ["host_metrics_format", "vector_metrics_format", "cadvisor_metrics_format", "mongodb_metrics_format", "redis_metrics_format", "caddy_metrics_format"]
endpoint = "https://oo2.prosopo.io/api/default/prometheus/api/v1/write"
auth.strategy = "basic"
auth.user = "${OO2_USERNAME:?err}"
auth.password = "${OO2_PASSWORD:?err}"
batch.timeout_secs = 15
request.rate_limit_duration_secs = 1
request.rate_limit_num = 10
buffer.type = "disk"
buffer.max_size = 268435488