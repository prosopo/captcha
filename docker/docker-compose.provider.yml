services:
    provider-dev:
        container_name: provider
        profiles:
            - development
        build:
            context: ..
            dockerfile: ./docker/images/provider.dockerfile
        env_file:
            - ../dev/scripts/.env.development
        ports:
            - '9229:9229'
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.2
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "curl", "--fail", "localhost:9229/v1/prosopo/provider/public/details" ]
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
    provider1:
        container_name: provider1
        profiles:
            - production
            - staging
        image: prosopo/provider:${COMPOSE_PROVIDER_IMAGE_VERSION}
        labels:
            - "vector.provider=true" # enable logging as a provider
            - "vector.docker=true" # log docker events
        restart: unless-stopped # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        env_file:
            - ../.env.1.${NODE_ENV}
        expose:
            - '9229'
        networks:
            internal:
                ipv4_address: 172.18.0.3
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "curl", "--fail", "localhost:9229/v1/prosopo/provider/public/details" ]
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
        stop_grace_period: 10s
        depends_on:
            database:
                condition: service_healthy
            caddy:
                condition: service_healthy
            vector:
                condition: service_healthy
    provider2:
        container_name: provider2
        profiles:
            - production
            - staging
        image: prosopo/provider:${COMPOSE_PROVIDER_PREVIOUS_IMAGE_VERSION}
        labels:
            - "vector.provider=true" # enable logging as a provider
            - "vector.docker=true" # log docker events
        restart: unless-stopped  # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        env_file:
            - ../.env.2.${NODE_ENV}
        expose:
            - '9339'
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.7
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "curl", "--fail", "localhost:9339/v1/prosopo/provider/public/details" ]
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
        stop_grace_period: 10s
        depends_on:
            database:
                condition: service_healthy
            caddy:
                condition: service_healthy
            vector:
                condition: service_healthy
    database-dev:
        container_name: database
        profiles:
            - development
        image: mongo:6.0.17
        # volumes:
        #   - ./db:/data/db
        ports:
            - '27017:27017'
        env_file:
            - ../dev/scripts/.env.development
        networks:
            internal:
                ipv4_address: 172.18.0.4
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "mongosh", "--eval", "db.runCommand({ ping: 1 })", "--quiet" ] # ping the mongo server
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
    database:
        container_name: database
        profiles:
            - production
            - staging
        image: mongo:6.0.17
        labels:
            - "vector.mongo=true" # enable logging as a provider
            - "vector.docker=true" # log docker events
        restart: unless-stopped # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        volumes:
            - /data/db:/data/db
        expose:
            - '27017'
        env_file:
            - ../.env.${NODE_ENV}
        networks:
            internal:
                ipv4_address: 172.18.0.5
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "mongosh", "--eval", "db.runCommand({ ping: 1 })", "--quiet" ] # ping the mongo server
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
        stop_grace_period: 10s
        depends_on:
            vector:
                condition: service_healthy
    caddy:
        container_name: caddy
        profiles:
            - production
            - staging
        image: prosopo/caddy:${CADDY_IMAGE}
        env_file:
            - ../.env.${NODE_ENV}
        labels:
            - "vector.caddy=true" # enable logging as caddy
            - "vector.docker=true" # log docker events
        restart: unless-stopped # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        ports:
            - '80:80'
            - '443:443'
            - '443:443/udp'
        volumes:
            - ./provider.Caddyfile:/etc/caddy/Caddyfile
            - caddy_data:/data
            - caddy_config:/config
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.6
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "curl", "--fail", "localhost:2020/metrics" ] # ping the caddy admin api
            interval: 5m
            retries: 3
            start_period: 30s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
        stop_grace_period: 10s
        depends_on:
            vector:
                condition: service_healthy
    vector:
        container_name: vector
        profiles:
            - production
            - staging
        image: prosopo/vector:${VECTOR_IMAGE}
        env_file:
            - ../.env.${NODE_ENV}
        labels:
            - "vector.docker=true" # log docker events
        restart: unless-stopped # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock # needed for monitoring docker container events, e.g. start/stop/etc
        networks:
            - internal
            - external
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: [ "CMD", "curl", "--fail", "localhost:8686/health" ]
            interval: 10s
            retries: 3
            start_period: 10s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
        stop_grace_period: 0s # will discard some logs that haven't been pushed up, but will stop the container immediately
networks:
    internal:
        name: internal
        internal: true
        driver: bridge
        ipam:
            config:
                - subnet: 172.18.0.0/16
                  gateway: 172.18.0.1
    external:
        name: external
        driver: bridge
        ipam:
            config:
                - subnet: 172.19.0.0/16
                  gateway: 172.19.0.1
volumes:
    caddy_data:
    caddy_config:
