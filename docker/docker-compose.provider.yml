services:
    provider-dev:
        container_name: provider
        profiles:
            - development
        build:
            context: ..
            dockerfile: ./docker/images/provider.dockerfile
        env_file:
            - ../dev/scripts/.env.development
        ports:
            - '9229:9229'
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.2
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "curl", "--fail", "localhost:9229/v1/prosopo/provider/status"]  # ping the status api
            interval: 30s
            retries: 3
            start_period: 30s
            timeout: 10s
    provider:
        container_name: provider
        profiles:
            - production
            - staging
        image: prosopo/provider:${COMPOSE_PROVIDER_IMAGE_VERSION}
        labels:
            - "vector.provider=true" # enable logging as a provider
            - "vector.docker=true" # log docker events
        restart: unless-stopped  # unless the container has been stopped, it will be restarted, even on reboot
        pull_policy: always
        env_file:
            - ../.env.${NODE_ENV}
        ports:
            - '9229:9229'
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.3
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "curl", "--fail", "localhost:9229/v1/prosopo/provider/details"]  # ping the details endpoint
            interval: 5m
            retries: 3
            start_period: 30s
            timeout: 10s
        dns:
            - 8.8.8.8
            - 1.1.1.1
            - 208.67.222.222
    database-dev:
        container_name: database
        profiles:
            - development
        image: mongo:6.0.17
        # volumes:
        #   - ./db:/data/db
        ports:
            - '27017:27017'
        env_file:
            - ../dev/scripts/.env.development
        networks:
            internal:
                ipv4_address: 172.18.0.4
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')", "--quiet"]  # ping the mongo server
            interval: 5m
            retries: 3
            start_period: 30s
            timeout: 10s
    database:
        container_name: database
        profiles:
            - production
            - staging
        image: mongo:6.0.17
        labels:
            - "vector.mongo=true" # enable logging as a provider
            - "vector.docker=true" # log docker events
        restart: unless-stopped  # unless the container has been stopped, it will be restarted, even on reboot
        volumes:
            - /data/db:/data/db
        ports:
            - '27017:27017'
        env_file:
            - ../.env.${NODE_ENV}
        networks:
            internal:
                ipv4_address: 172.18.0.5
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "mongo", "--eval", "db.adminCommand('ping')", "--quiet"]  # ping the mongo server
            interval: 5m
            retries: 3
            start_period: 30s
            timeout: 10s
    caddy:
        container_name: caddy
        profiles:
            - production
            - staging
        image: prosopo/caddy:latest
        env_file:
            - ../.env.${NODE_ENV}
        labels:
            - "vector.caddy=true" # enable logging as caddy
            - "vector.docker=true" # log docker events
        restart: unless-stopped  # unless the container has been stopped, it will be restarted, even on reboot
        ports:
            - '80:80'
            - '443:443'
            - '443:443/udp'
        volumes:
            - ./provider.Caddyfile:/etc/caddy/Caddyfile
            - caddy_data:/data
            - caddy_config:/config
        networks:
            external:
            internal:
                ipv4_address: 172.18.0.6
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "curl", "--fail", "localhost:2019/metrics"]  # ping the caddy admin api
            interval: 5m
            retries: 3
            start_period: 30s
            timeout: 10s
    watchtower:
        container_name: watchtower
        profiles:
            - production
            - staging
        image: containrrr/watchtower
        command: ["--log-format", "JSON", "--remove-volumes", "--cleanup", "--warn-on-head-failure", "never", "--interval", "30", "--include-restarting"]
        labels:
            - "vector.watchtower=true" # enable logging as a watchtower
            - "vector.docker=true" # log docker events
        restart: unless-stopped  # unless the container has been stopped, it will be restarted, even on reboot
        env_file:
            - ../.env.${NODE_ENV}
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
    vector:
        container_name: vector
        profiles:
            - production
            - staging
        image: prosopo/vector:${COMPOSE_PROVIDER_IMAGE_VERSION}
        env_file:
            - ../.env.${NODE_ENV}
        labels:
            - "vector.docker=true" # log docker events
        restart: unless-stopped # unless the container has been stopped, it will be restarted, even on reboot
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock # needed for monitoring docker container events, e.g. start/stop/etc
        networks:
            - internal
            - external
        logging:
            driver: 'json-file'
            options:
                max-size: '100m'
                max-file: '1'
        healthcheck:
            test: ["CMD", "curl", "--fail", "localhost:8686/health"]
            interval: 30s
            timeout: 10s
            retries: 1
            start_period: 10s
networks:
    internal:
        name: internal
        internal: true
        driver: bridge
        ipam:
            config:
                - subnet: 172.18.0.0/16
                  gateway: 172.18.0.1
    external:
        name: external
volumes:
    caddy_data:
    caddy_config:
